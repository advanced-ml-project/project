{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20698, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>vaderMean</th>\n",
       "      <th>vaderStd</th>\n",
       "      <th>vaderCatLabel</th>\n",
       "      <th>vaderCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1377385383168765952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FoxNews</td>\n",
       "      <td>activists protest renaming chicago school afte...</td>\n",
       "      <td>306</td>\n",
       "      <td>-0.052830</td>\n",
       "      <td>0.445459</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1377384607969013765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FoxNews</td>\n",
       "      <td>border patrol video shows smugglers abandoning...</td>\n",
       "      <td>108</td>\n",
       "      <td>-0.045958</td>\n",
       "      <td>0.495337</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1377384339105669122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FoxNews</td>\n",
       "      <td>cause of tiger woods car crash determined but ...</td>\n",
       "      <td>169</td>\n",
       "      <td>-0.034919</td>\n",
       "      <td>0.424833</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1377367836046192641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FoxNews</td>\n",
       "      <td>gop rep urges hhs to halt reported plan to rel...</td>\n",
       "      <td>80</td>\n",
       "      <td>0.043459</td>\n",
       "      <td>0.495874</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1377358399759785987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FoxNews</td>\n",
       "      <td>some democrats trying to stop iowa new hampshi...</td>\n",
       "      <td>96</td>\n",
       "      <td>-0.040135</td>\n",
       "      <td>0.433053</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  topic   source  \\\n",
       "0  1377385383168765952    NaN  FoxNews   \n",
       "1  1377384607969013765    NaN  FoxNews   \n",
       "2  1377384339105669122    NaN  FoxNews   \n",
       "3  1377367836046192641    NaN  FoxNews   \n",
       "4  1377358399759785987    NaN  FoxNews   \n",
       "\n",
       "                                                text  replyCount  vaderMean  \\\n",
       "0  activists protest renaming chicago school afte...         306  -0.052830   \n",
       "1  border patrol video shows smugglers abandoning...         108  -0.045958   \n",
       "2  cause of tiger woods car crash determined but ...         169  -0.034919   \n",
       "3  gop rep urges hhs to halt reported plan to rel...          80   0.043459   \n",
       "4  some democrats trying to stop iowa new hampshi...          96  -0.040135   \n",
       "\n",
       "   vaderStd vaderCatLabel  vaderCat  \n",
       "0  0.445459        medium       1.0  \n",
       "1  0.495337        medium       1.0  \n",
       "2  0.424833        medium       1.0  \n",
       "3  0.495874        medium       1.0  \n",
       "4  0.433053        medium       1.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('data/tweets_w_scores.csv')\n",
    "print(tweets.shape)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vaderCat\n",
       "0.0     5804\n",
       "1.0    10094\n",
       "2.0     4800\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby(by='vaderCat').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, remove_stopwords=True):\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Tokenize each word\n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [activists, protest, renaming, chicago, school...\n",
       "1        [border, patrol, video, shows, smugglers, aban...\n",
       "2        [cause, tiger, woods, car, crash, determined, ...\n",
       "3        [gop, rep, urges, hhs, halt, reported, plan, r...\n",
       "4        [democrats, trying, stop, iowa, new, hampshire...\n",
       "                               ...                        \n",
       "20693    [u, n, special, envoy, tells, security, counci...\n",
       "20694    [wisconsin, high, court, voids, governors, mas...\n",
       "20695    [analysis, biden, infrastructure, plan, bets, ...\n",
       "20696    [analysis, deliveroos, flop, wake, call, tech,...\n",
       "20697    [defense, tells, canada, court, huawei, cfos, ...\n",
       "Name: tokens, Length: 20698, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tokens'] = tweets['text'].astype(str).apply(tokenize_text)\n",
    "tweets['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14488, 10)\n",
      "(6210, 10)\n"
     ]
    }
   ],
   "source": [
    "training_data, test_data = train_test_split(tweets, train_size = 0.7, random_state=42)\n",
    "y_tr = training_data['vaderCat']\n",
    "y_te = test_data['vaderCat']\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(train, test, description):\n",
    "    '''\n",
    "    Extracts features for Bag of words, Bag of n-grams \n",
    "    or Tf-Idf\n",
    "    '''\n",
    "    \n",
    "    if description == \"bow\" or description == \"tfidf\":\n",
    "        transform = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "        X_tr = transform.fit_transform(train)\n",
    "        X_te = transform.transform(test)\n",
    "        print(description, ': the size of the voc is', len(transform.vocabulary_))\n",
    "        \n",
    "    elif description == \"ngram\":\n",
    "        transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[3,3], lowercase=False)\n",
    "        X_tr = transform.fit_transform(train)\n",
    "        X_te = transform.transform(test)\n",
    "        print(description, ': the size of the voc is', len(transform.vocabulary_))\n",
    "        \n",
    "        \n",
    "    if description == \"tfidf\":\n",
    "        transform = text.TfidfTransformer(norm=None)\n",
    "        X_tr = transform.fit_transform(X_tr)\n",
    "        X_te = transform.transform(X_te) \n",
    "        \n",
    "    print('The shape of the df is:', X_tr.shape)   \n",
    "    \n",
    "    return X_tr, X_te, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow : the size of the voc is 18955\n",
      "The shape of the df is: (14488, 18955)\n",
      "ngram : the size of the voc is 134805\n",
      "The shape of the df is: (14488, 134805)\n",
      "tfidf : the size of the voc is 18955\n",
      "The shape of the df is: (14488, 18955)\n"
     ]
    }
   ],
   "source": [
    "X_tr_bow, X_te_bow, tr_bow = feature_extraction(training_data['tokens'], test_data['tokens'], \"bow\")\n",
    "X_tr_ngram, X_te_ngram, tr_ngram = feature_extraction(training_data['tokens'], test_data['tokens'], \"ngram\")\n",
    "X_tr_tfidf, X_te_tfidf, tr_tfidf = feature_extraction(training_data['tokens'], test_data['tokens'], \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aapi community indeed',\n",
       " 'aapi hate group',\n",
       " 'aardvark local monsters',\n",
       " 'aari mcdonald carried',\n",
       " 'aaron donald attacked',\n",
       " 'aaron donalds attorney',\n",
       " 'aaron rodgers packers',\n",
       " 'aaron rodgers still',\n",
       " 'aaron wrote nasty',\n",
       " 'ababu negash said',\n",
       " 'ababu said town',\n",
       " 'aback find fellow',\n",
       " 'abajo durante cinco',\n",
       " 'abandon claims u',\n",
       " 'abandon measures global',\n",
       " 'abandoned apartment blocks',\n",
       " 'abandoned breakaway project',\n",
       " 'abandoned campfire driven',\n",
       " 'abandoned contentious plan',\n",
       " 'abandoned donkeys pampered',\n",
       " 'abandoned mannequins price',\n",
       " 'abandoned neighbors fled',\n",
       " 'abandoned pets refers',\n",
       " 'abandoned plans build',\n",
       " 'abandoned push neera',\n",
       " 'abandoned soccer competition',\n",
       " 'abandoned three years',\n",
       " 'abandoned town pripyat',\n",
       " 'abandoned vessel adrift']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_ngram.get_feature_names()[1:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_ = {'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}\n",
    "\n",
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0):\n",
    "    model = LogisticRegression(multi_class=\"multinomial\", C=_C).fit(X_tr, y_tr)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('Test Score with', description, 'features', score)\n",
    "    return model\n",
    "\n",
    "def grid_logistic_classify(X_tr, y_tr, X_test, y_test, description, param_grid=param_grid_):\n",
    "    grid = sklearn.model_selection.GridSearchCV(LogisticRegression(multi_class=\"multinomial\"), cv=5, param_grid=param_grid_)\n",
    "    model = grid.fit(X_tr, y_tr)\n",
    "    score = model.best_estimator_.score(X_test, y_test)\n",
    "    print('Test Score with', description, 'features', score)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score with bow features 0.46296296296296297\n",
      "Test Score with n-grams features 0.4877616747181965\n",
      "Test Score with tfidf features 0.42962962962962964\n"
     ]
    }
   ],
   "source": [
    "model_bow = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow')\n",
    "model_bow_tr = simple_logistic_classify(X_tr_ngram, y_tr, X_te_ngram, y_te, 'n-grams')\n",
    "model_tfidf_tr = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vaderCat\n",
       "0.0    1722\n",
       "1.0    3068\n",
       "2.0    1420\n",
       "dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.groupby(by='vaderCat').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_pred\n",
       "0.0    1717\n",
       "1.0    3041\n",
       "2.0    1452\n",
       "dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['y_pred'] = model_tfidf_tr.predict(X_te_tfidf)\n",
    "test_data.groupby(by='y_pred').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 622  748  352]\n",
      " [ 766 1624  678]\n",
      " [ 329  669  422]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.3612079 , 0.52933507, 0.2971831 ])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy by category\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#test_data.loc[test_data['vaderCat'] != test_data['y_pred']].groupby(by='vaderCat').size()\n",
    "matrix = confusion_matrix(y_te, test_data['y_pred'])\n",
    "print(matrix)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the cross-entropy loss if the ‘multi_class’ option is set to ‘multinomial’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most important features category 0\n",
      "--------> bow\n",
      "['expelled', 'trans', 'pull', 'sparked', 'distancing', 'prayers', 'snapped', 'rally', 'switzerland', 'seed', 'urging', 'placebo', 'massing', 'chains', 'drives', 'ontario', 'surviving', 'resolve', 'void', 'grandfather']\n",
      "--------> ngram\n",
      "['pfizer covid vaccine', 'icymi jenny narumi', 'winners bafta film', 'bafta film awards', 'indias daily covid', 'new mainland covid', 'mainland covid cases', 'reports new mainland', 'icymi greenland huge', 'covid cases hours', 'suez canal says', 'icymi microsoft talks', 'india reports record', 'tells fell horse', 'putin tells fell', 'biontech covid vaccine', 'icymi russias rugball', 'people intensive care', 'million first time', 'international space station']\n"
     ]
    }
   ],
   "source": [
    "#class 0\n",
    "print('most important features category 0')\n",
    "print('--------> bow')\n",
    "bow_coef0 = model_bow.coef_[0]\n",
    "imp0 = bow_coef0.argsort()[-20:][::-1]\n",
    "features = tr_bow.get_feature_names()\n",
    "print([features[index] for index in imp0])\n",
    "print('--------> ngram')\n",
    "ngram_coef0 = model_bow_tr.coef_[0]\n",
    "imp0 = ngram_coef0.argsort()[-20:][::-1]\n",
    "features = tr_ngram.get_feature_names()\n",
    "print([features[index] for index in imp0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most important features category 0\n",
      "--------> bow\n",
      "['driver', 'reactions', 'contaminated', 'filibuster', 'gop', 'pepper', 'cooking', 'surges', 'existing', 'bbc', 'narrow', 'quarantine', 'slams', 'seeing', 'aspiring', 'afternoon', 'prisons', 'disperse', 'might', 'kyodo']\n",
      "--------> ngram\n",
      "['breaking news u', 'gov andrew cuomo', 'factbox reactions prince', 'reactions prince harry', 'mainland china reports', 'election reform bill', 'president donald trump', 'secretary state blinken', 'icymi nike sued', 'bitcoin rises percent', 'need know coronavirus', 'watch feeling lonely', 'icymi indias northern', 'icymi harley davidson', 'icymi nearly million', 'watch india ready', 'meghan duchess sussex', 'watch surging demand', 'shooting daunte wright', 'icymi hundreds german']\n"
     ]
    }
   ],
   "source": [
    "#class 1\n",
    "print('most important features category 0')\n",
    "print('--------> bow')\n",
    "bow_coef0 = model_bow.coef_[1]\n",
    "imp0 = bow_coef0.argsort()[-20:][::-1]\n",
    "features = tr_bow.get_feature_names()\n",
    "print([features[index] for index in imp0])\n",
    "print('--------> ngram')\n",
    "bow_coef0 = model_bow_tr.coef_[1]\n",
    "imp0 = bow_coef0.argsort()[-20:][::-1]\n",
    "features = tr_ngram.get_feature_names()\n",
    "print([features[index] for index in imp0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most important features category 0\n",
      "--------> bow\n",
      "['aged', 'love', 'frustration', 'rd', 'apologises', 'mayorkas', 'programme', 'firms', 'kurdish', 'invite', 'quad', 'sagawards', 'cannon', 'disaster', 'govt', 'extremism', 'counted', 'continued', 'crackdown', 'serve']\n",
      "--------> ngram\n",
      "['watch president joe', 'derek chauvin trial', 'icymi berlin zoo', 'icymi year old', 'derek chauvins trial', 'near kings palace', 'helicopter crash sunday', 'todays great read', 'behold war tuba', 'experience entire series', 'says hong kong', 'saudi crown prince', 'watch heres look', 'pro kurdish party', 'italy recommends astrazeneca', 'recommends astrazeneca covid', 'icymi former bachelor', 'records new covid', 'daily record covid', 'china announces sanctions']\n"
     ]
    }
   ],
   "source": [
    "#class 2\n",
    "print('most important features category 0')\n",
    "print('--------> bow')\n",
    "bow_coef0 = model_bow.coef_[2]\n",
    "imp0 = bow_coef0.argsort()[-20:][::-1]\n",
    "features = tr_bow.get_feature_names()\n",
    "print([features[index] for index in imp0])\n",
    "print('--------> ngram')\n",
    "bow_coef0 = model_bow_tr.coef_[2]\n",
    "imp0 = bow_coef0.argsort()[-20:][::-1]\n",
    "features = tr_ngram.get_feature_names()\n",
    "print([features[index] for index in imp0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4613526570048309"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW\n",
    "pipeline = make_pipeline((SMOTE(random_state=0)), LogisticRegression(multi_class=\"multinomial\", random_state=0))\n",
    "#model\n",
    "params = {'logisticregression__penalty': ['l2'],\n",
    "          'logisticregression__C': [0.01, 0.1, 1, 10, 100],\n",
    "          'logisticregression__solver': ['lbfgs']}\n",
    "\n",
    "grid_bow = GridSearchCV(estimator=pipeline,\n",
    "                    param_grid=params,\n",
    "                    cv=10,\n",
    "                    return_train_score=True,\n",
    "                    #scoring= ['accuracy', 'precision', 'recall'],\n",
    "                    #refit = 'recall'\n",
    "                     )\n",
    "\n",
    "grid_bow.fit(X_tr_bow, y_tr)\n",
    "score = grid_bow.best_estimator_.score(X_te_bow, y_te)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3404186795491143"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW\n",
    "pipeline = make_pipeline((SMOTE(random_state=0)), LogisticRegression(multi_class=\"multinomial\", random_state=0))\n",
    "#model\n",
    "params = {'logisticregression__penalty': ['l2'],\n",
    "          'logisticregression__C': [0.01, 0.1, 1, 10, 100],\n",
    "          'logisticregression__solver': ['lbfgs']}\n",
    "\n",
    "grid_ngram = GridSearchCV(estimator=pipeline,\n",
    "                    param_grid=params,\n",
    "                    cv=10,\n",
    "                    return_train_score=True,\n",
    "                    #scoring= ['accuracy', 'precision', 'recall'],\n",
    "                    #refit = 'recall'\n",
    "                     )\n",
    "\n",
    "grid_ngram.fit(X_tr_ngram, y_tr)\n",
    "score = grid_ngram.best_estimator_.score(X_te_ngram, y_te)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4454106280193237"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW\n",
    "pipeline = make_pipeline((SMOTE(random_state=0)), LogisticRegression(multi_class=\"multinomial\", random_state=0))\n",
    "#model\n",
    "params = {'logisticregression__penalty': ['l2'],\n",
    "          'logisticregression__C': [0.01, 0.1, 1, 10, 100],\n",
    "          'logisticregression__solver': ['lbfgs']}\n",
    "\n",
    "grid_tfidf = GridSearchCV(estimator=pipeline,\n",
    "                    param_grid=params,\n",
    "                    cv=10,\n",
    "                    return_train_score=True,\n",
    "                    #scoring= ['accuracy', 'precision', 'recall'],\n",
    "                    #refit = 'recall'\n",
    "                     )\n",
    "\n",
    "grid_tfidf.fit(X_tr_tfidf, y_tr)\n",
    "score = grid_tfidf.best_estimator_.score(X_te_tfidf, y_te)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_pred\n",
       "0.0    1733\n",
       "1.0    3027\n",
       "2.0    1450\n",
       "dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['y_pred'] = grid_tfidf.best_estimator_.predict(X_te_tfidf)\n",
    "test_data.groupby(by='y_pred').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 658  730  334]\n",
      " [ 738 1661  669]\n",
      " [ 337  636  447]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.38211382, 0.54139505, 0.31478873])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy by category tfidf\n",
    "matrix = confusion_matrix(y_te, test_data['y_pred'])\n",
    "print(matrix)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score with bow features 0.49404186795491145\n",
      "Test Score with n-grams features 0.49404186795491145\n",
      "Test Score with tfidf features 0.5027375201288244\n"
     ]
    }
   ],
   "source": [
    "model_bow = grid_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow')\n",
    "model_bow_tr = grid_logistic_classify(X_tr_ngram, y_tr, X_te_ngram, y_te, 'n-grams')\n",
    "model_tfidf_transform = grid_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1e-05}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bow_tr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_bow.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_pred\n",
       "0.0    1013\n",
       "1.0    4745\n",
       "2.0     452\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['y_pred'] = model_tfidf_transform.best_estimator_.predict(X_te_tfidf)\n",
    "test_data.groupby(by='y_pred').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 443 1212   67]\n",
      " [ 382 2490  196]\n",
      " [ 188 1043  189]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.257259  , 0.81160365, 0.13309859])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy by category\n",
    "#test_data.loc[test_data['vaderCat'] != test_data['y_pred']].groupby(by='vaderCat').size()\n",
    "matrix = confusion_matrix(y_te, test_data['y_pred'])\n",
    "print(matrix)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
