{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34731, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>vaderCatLabel</th>\n",
       "      <th>vaderCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1377385383168765952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FoxNews</td>\n",
       "      <td>activists protest renaming chicago school afte...</td>\n",
       "      <td>306</td>\n",
       "      <td>high</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1377384607969013765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FoxNews</td>\n",
       "      <td>border patrol video shows smugglers abandoning...</td>\n",
       "      <td>108</td>\n",
       "      <td>high</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1377384339105669122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FoxNews</td>\n",
       "      <td>cause of tiger woods car crash determined but ...</td>\n",
       "      <td>169</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1377367836046192641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FoxNews</td>\n",
       "      <td>gop rep urges hhs to halt reported plan to rel...</td>\n",
       "      <td>80</td>\n",
       "      <td>high</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1377358399759785987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FoxNews</td>\n",
       "      <td>some democrats trying to stop iowa new hampshi...</td>\n",
       "      <td>96</td>\n",
       "      <td>high</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  topic   source  \\\n",
       "0  1377385383168765952    NaN  FoxNews   \n",
       "1  1377384607969013765    NaN  FoxNews   \n",
       "2  1377384339105669122    NaN  FoxNews   \n",
       "3  1377367836046192641    NaN  FoxNews   \n",
       "4  1377358399759785987    NaN  FoxNews   \n",
       "\n",
       "                                                text  replyCount  \\\n",
       "0  activists protest renaming chicago school afte...         306   \n",
       "1  border patrol video shows smugglers abandoning...         108   \n",
       "2  cause of tiger woods car crash determined but ...         169   \n",
       "3  gop rep urges hhs to halt reported plan to rel...          80   \n",
       "4  some democrats trying to stop iowa new hampshi...          96   \n",
       "\n",
       "  vaderCatLabel  vaderCat  \n",
       "0          high       1.0  \n",
       "1          high       1.0  \n",
       "2           low       0.0  \n",
       "3          high       1.0  \n",
       "4          high       1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('data/tweets_all.csv')\n",
    "print(tweets.shape)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vaderCat\n",
       "0.0    15062\n",
       "1.0    19669\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby(by='vaderCat').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, remove_stopwords=True):\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Tokenize each word\n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [activists, protest, renaming, chicago, school...\n",
       "1        [border, patrol, video, shows, smugglers, aban...\n",
       "2        [cause, tiger, woods, car, crash, determined, ...\n",
       "3        [gop, rep, urges, hhs, halt, reported, plan, r...\n",
       "4        [democrats, trying, stop, iowa, new, hampshire...\n",
       "                               ...                        \n",
       "34726    [much, share, trump, celebrates, acquittal, se...\n",
       "34727    [senate, majority, leader, chuck, schumer, bla...\n",
       "34728    [breaking, u, senate, acquits, trump, seven, r...\n",
       "34729    [outcome, clear, final, verdict, rendered, rep...\n",
       "34730                         [breaking, trump, acquitted]\n",
       "Name: tokens, Length: 34731, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tokens'] = tweets['text'].astype(str).apply(tokenize_text)\n",
    "tweets['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24311, 8)\n",
      "(10420, 8)\n"
     ]
    }
   ],
   "source": [
    "training_data, test_data = train_test_split(tweets, train_size = 0.7, random_state=42)\n",
    "y_tr = training_data['vaderCat']\n",
    "y_te = test_data['vaderCat']\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(train, test, description):\n",
    "    '''\n",
    "    Extracts features for Bag of words, Bag of n-grams \n",
    "    or Tf-Idf\n",
    "    '''\n",
    "    \n",
    "    if description == \"bow\" or description == \"tfidf\":\n",
    "        transform = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "        X_tr = transform.fit_transform(train)\n",
    "        X_te = transform.transform(test)\n",
    "        print(description, ': the size of the voc is', len(transform.vocabulary_))\n",
    "        \n",
    "    elif description == \"ngram\":\n",
    "        transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[3,3], lowercase=False)\n",
    "        X_tr = transform.fit_transform(train)\n",
    "        X_te = transform.transform(test)\n",
    "        print(description, ': the size of the voc is', len(transform.vocabulary_))\n",
    "        \n",
    "        \n",
    "    if description == \"tfidf\":\n",
    "        transform = text.TfidfTransformer(norm=None)\n",
    "        X_tr = transform.fit_transform(X_tr)\n",
    "        X_te = transform.transform(X_te) \n",
    "        \n",
    "    print('The shape of the df is:', X_tr.shape)   \n",
    "    \n",
    "    return X_tr, X_te, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow : the size of the voc is 23638\n",
      "The shape of the df is: (24311, 23638)\n",
      "ngram : the size of the voc is 261332\n",
      "The shape of the df is: (24311, 261332)\n",
      "tfidf : the size of the voc is 23638\n",
      "The shape of the df is: (24311, 23638)\n"
     ]
    }
   ],
   "source": [
    "X_tr_bow, X_te_bow, tr_bow = feature_extraction(training_data['tokens'], test_data['tokens'], \"bow\")\n",
    "X_tr_ngram, X_te_ngram, tr_ngram = feature_extraction(training_data['tokens'], test_data['tokens'], \"ngram\")\n",
    "X_tr_tfidf, X_te_tfidf, tr_tfidf = feature_extraction(training_data['tokens'], test_data['tokens'], \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_ = {'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}\n",
    "\n",
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0):\n",
    "    model = LogisticRegression(C=_C).fit(X_tr, y_tr)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('Test Score with', description, 'features', score)\n",
    "    return model\n",
    "\n",
    "def grid_logistic_classify(X_tr, y_tr, X_test, y_test, description, param_grid=param_grid_):\n",
    "    grid = GridSearchCV(LogisticRegression(), cv=5, param_grid=param_grid_)\n",
    "    model = grid.fit(X_tr, y_tr)\n",
    "    score = model.best_estimator_.score(X_test, y_test)\n",
    "    print('Test Score with', description, 'features', score)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score with bow features 0.6108445297504799\n",
      "Test Score with n-grams features 0.5938579654510556\n",
      "Test Score with tfidf features 0.572168905950096\n"
     ]
    }
   ],
   "source": [
    "model_bow = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow')\n",
    "model_bow_tr = simple_logistic_classify(X_tr_ngram, y_tr, X_te_ngram, y_te, 'n-grams')\n",
    "model_tfidf_tr = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vaderCat\n",
       "0.0    4496\n",
       "1.0    5924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.groupby(by='vaderCat').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_pred\n",
       "0.0    4524\n",
       "1.0    5896\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['y_pred'] = model_tfidf_tr.predict(X_te_tfidf)\n",
    "test_data.groupby(by='y_pred').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2281 2215]\n",
      " [2243 3681]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.50733986, 0.6213707 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy by category\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#test_data.loc[test_data['vaderCat'] != test_data['y_pred']].groupby(by='vaderCat').size()\n",
    "matrix = confusion_matrix(y_te, test_data['y_pred'])\n",
    "print(matrix)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the cross-entropy loss if the ‘multi_class’ option is set to ‘multinomial’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most important features\n",
      "--------> bow\n",
      "['towns', 'everybody', 'sending', 'equality', 'pardons', 'dismissal', 'blake', 'significantly', 'moratorium', 'referring', 'panther', 'anticipation', 'tank', 'fedex', 'desantis', 'soft', 'entering', 'combination', 'vatican', 'midwest']\n",
      "--------> ngram\n",
      "['impeach president trump', 'house impeachment managers', 'derek chauvin trial', 'joe bidens election', 'breaking news president', 'donald trump said', 'joe bidens win', 'black lives matter', 'national security law', 'opinion joe bidens', 'former president trump', 'us mexico border', 'writes harry enten', 'president trump says', 'meghan duchess sussex', 'capitol police officer', 'tested positive coronavirus', 'president trump said', 'president trumps campaign', 'first presidential debate']\n"
     ]
    }
   ],
   "source": [
    "#class 0\n",
    "print('most important features')\n",
    "print('--------> bow')\n",
    "bow_coef0 = model_bow.coef_[0]\n",
    "imp0 = bow_coef0.argsort()[-20:][::-1]\n",
    "features = tr_bow.get_feature_names()\n",
    "print([features[index] for index in imp0])\n",
    "print('--------> ngram')\n",
    "ngram_coef0 = model_bow_tr.coef_[0]\n",
    "imp0 = ngram_coef0.argsort()[-20:][::-1]\n",
    "features = tr_ngram.get_feature_names()\n",
    "print([features[index] for index in imp0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6271593090211133"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW\n",
    "pipeline = make_pipeline((SMOTE(random_state=0)), LogisticRegression(random_state=0))\n",
    "#model\n",
    "params = {'logisticregression__penalty': ['l2'],\n",
    "          'logisticregression__C': [0.01, 0.1, 1, 10, 100],\n",
    "          'logisticregression__solver': ['lbfgs']}\n",
    "\n",
    "grid_bow = GridSearchCV(estimator=pipeline,\n",
    "                    param_grid=params,\n",
    "                    cv=10,\n",
    "                    return_train_score=True,\n",
    "                    #scoring= ['accuracy', 'precision', 'recall'],\n",
    "                    #refit = 'recall'\n",
    "                     )\n",
    "\n",
    "grid_bow.fit(X_tr_bow, y_tr)\n",
    "score = grid_bow.best_estimator_.score(X_te_bow, y_te)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5147792706333973"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW\n",
    "pipeline = make_pipeline((SMOTE(random_state=0)), LogisticRegression(multi_class=\"multinomial\", random_state=0))\n",
    "#model\n",
    "params = {'logisticregression__penalty': ['l2'],\n",
    "          'logisticregression__C': [0.01, 0.1, 1, 10, 100],\n",
    "          'logisticregression__solver': ['lbfgs']}\n",
    "\n",
    "grid_ngram = GridSearchCV(estimator=pipeline,\n",
    "                    param_grid=params,\n",
    "                    cv=10,\n",
    "                    return_train_score=True,\n",
    "                    #scoring= ['accuracy', 'precision', 'recall'],\n",
    "                    #refit = 'recall'\n",
    "                     )\n",
    "\n",
    "grid_ngram.fit(X_tr_ngram, y_tr)\n",
    "score = grid_ngram.best_estimator_.score(X_te_ngram, y_te)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5995201535508637"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW\n",
    "pipeline = make_pipeline((SMOTE(random_state=0)), LogisticRegression(multi_class=\"multinomial\", random_state=0))\n",
    "#model\n",
    "params = {'logisticregression__penalty': ['l2'],\n",
    "          'logisticregression__C': [0.01, 0.1, 1, 10, 100],\n",
    "          'logisticregression__solver': ['lbfgs']}\n",
    "\n",
    "grid_tfidf = GridSearchCV(estimator=pipeline,\n",
    "                    param_grid=params,\n",
    "                    cv=10,\n",
    "                    return_train_score=True,\n",
    "                    #scoring= ['accuracy', 'precision', 'recall'],\n",
    "                    #refit = 'recall'\n",
    "                     )\n",
    "\n",
    "grid_tfidf.fit(X_tr_tfidf, y_tr)\n",
    "score = grid_tfidf.best_estimator_.score(X_te_tfidf, y_te)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_pred\n",
       "0.0    4697\n",
       "1.0    5723\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['y_pred'] = grid_tfidf.best_estimator_.predict(X_te_tfidf)\n",
    "test_data.groupby(by='y_pred').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2510 1986]\n",
      " [2187 3737]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.55827402, 0.63082377])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy by category tfidf\n",
    "matrix = confusion_matrix(y_te, test_data['y_pred'])\n",
    "print(matrix)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search to tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score with bow features 0.6385796545105566\n",
      "Test Score with n-grams features 0.5938579654510556\n",
      "Test Score with tfidf features 0.6458733205374281\n"
     ]
    }
   ],
   "source": [
    "model_bow = grid_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow')\n",
    "model_bow_tr = grid_logistic_classify(X_tr_ngram, y_tr, X_te_ngram, y_te, 'n-grams')\n",
    "model_tfidf_transform = grid_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.001}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tfidf_transform.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_bow.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_pred\n",
       "0.0    3606\n",
       "1.0    6814\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['y_pred'] = model_tfidf_transform.best_estimator_.predict(X_te_tfidf)\n",
    "test_data.groupby(by='y_pred').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2206 2290]\n",
      " [1400 4524]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.49065836, 0.76367319])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy by category\n",
    "#test_data.loc[test_data['vaderCat'] != test_data['y_pred']].groupby(by='vaderCat').size()\n",
    "matrix = confusion_matrix(y_te, test_data['y_pred'])\n",
    "print(matrix)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7103155911446065"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(test_data['y_pred'], y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most important features\n",
      "--------> bow\n",
      "['opinion', 'meghan', 'trumps', 'trump', 'pardons', 'book', 'nuclear', 'sending', 'challenges', 'offices', 'violent', 'arrested', 'amid', 'reporting', 'saturday', 'original', 'ship', 'dog', 'vatican', 'recount']\n",
      "--------> ngram\n",
      "['watch president elect', 'impeach president trump', 'house impeachment managers', 'sputnik v vaccine', 'derek chauvin trial', 'icymi president joe', 'stuck suez canal', 'joe bidens election', 'european super league', 'breaking news president', 'donald trump said', 'joe bidens win', 'five things watch', 'black lives matter', 'national security law', 'breaking joe biden', 'british prime minister', 'indias daily covid', 'biden infrastructure plan', 'opinion joe bidens']\n",
      "--------> tfidf\n",
      "['trump', 'trumps', 'opinion', 'biden', 'president', 'us', 'meghan', 'arrested', 'writes', 'amid', 'analysis', 'impeachment', 'democrats', 'icymi', 'book', 'republicans', 'nuclear', 'bidens', 'pardons', 'challenges']\n"
     ]
    }
   ],
   "source": [
    "#class 0\n",
    "print('most important features')\n",
    "print('--------> bow')\n",
    "bow_coef0 = abs(model_bow.best_estimator_.coef_[0])\n",
    "imp0 = bow_coef0.argsort()[-20:][::-1]\n",
    "features = tr_bow.get_feature_names()\n",
    "print([features[index] for index in imp0])\n",
    "print('--------> ngram')\n",
    "ngram_coef0 = abs(model_bow_tr.best_estimator_.coef_[0])\n",
    "imp0 = ngram_coef0.argsort()[-20:][::-1]\n",
    "features = tr_ngram.get_feature_names()\n",
    "print([features[index] for index in imp0])\n",
    "print('--------> tfidf')\n",
    "ngram_coef0 = abs(model_tfidf_transform.best_estimator_.coef_[0])\n",
    "imp0 = ngram_coef0.argsort()[-20:][::-1]\n",
    "features = tr_bow.get_feature_names()\n",
    "print([features[index] for index in imp0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
