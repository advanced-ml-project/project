{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ebe268",
   "metadata": {},
   "source": [
    "# Using snscraper to Get Tweets\n",
    "\n",
    "## Step 1: Make sure you are running python v. 3.8 or higher.\n",
    "If you are not, to update python try:\n",
    "!conda upgrade notebook \n",
    "or \n",
    "!conda update jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02fde4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.2\n"
     ]
    }
   ],
   "source": [
    "# check version\n",
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0abaa7b",
   "metadata": {},
   "source": [
    "## Step 2: First time use requires installation of dev version of snsscrape\n",
    "Uncomment and run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c9c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759cdb90",
   "metadata": {},
   "source": [
    "## Step 3: Imports and Functions\n",
    "Please run the code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5dd096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a898b",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4fec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosts(newsOutlet, maxTweets, start, end):\n",
    "    '''\n",
    "    Based on MartinBeckUT's python wrapper for snscraper.\n",
    "    Get all tweets and replies from March 2021.\n",
    "    \n",
    "    Inputs:\n",
    "        - newsOutlet: string, name of twitter news source user\n",
    "        - maxTweets: int, cut off for search results\n",
    "        - start: string ('YYYY-MM-DD'), first date to include in search\n",
    "        - end: string ('YYYY-MM-DD'), first date to exclude in search\n",
    "        \n",
    "    Returns: tuple ( tweets_df, replies_list )\n",
    "        - tweets_df: a pandas dataframe with columns: \n",
    "            Datetime,TweetId, text, userName, newsOutlet, \n",
    "            MentionedUsers, ConversationId\n",
    "        - replies_list: list, all convesationIds from the tweets\n",
    "        \n",
    "    '''     \n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list = []\n",
    "    replies_list = []\n",
    "\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i, tweet in enumerate(sntwitter\n",
    "                              .TwitterSearchScraper(f'from:{ newsOutlet } since:{start} ' +\n",
    "                                               f'until:{end} -is:retweet -is:reply')\n",
    "                              .get_items()):\n",
    "        if i>maxTweets:\n",
    "            break\n",
    "            \n",
    "        mentions = ''\n",
    "\n",
    "        if tweet.id != tweet.conversationId:\n",
    "            if tweet.mentionedUsers:\n",
    "                for user in tweet.mentionedUsers:\n",
    "                    mentions += (' ' + user.username)\n",
    "                else:\n",
    "                    mentions = ''\n",
    "\n",
    "        text = tweet.content\n",
    "        text = text.replace(',', '')\n",
    "        text = text.replace('\"', '')\n",
    "        text = text.replace(\"'\", '')\n",
    "        \n",
    "        tweets_list.append([tweet.date, \n",
    "                            str(tweet.id), \n",
    "                            text, \n",
    "                            tweet.user.username,\n",
    "                            mentions,\n",
    "                            str(tweet.conversationId)])\n",
    "        \n",
    "        if tweet.conversationId not in replies_list:\n",
    "            replies_list.append(tweet.conversationId)\n",
    "    \n",
    "    # Creating a dataframes from the tweets lists above\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', \n",
    "                                                    'TweetId', \n",
    "                                                    'Text', \n",
    "                                                    'Username',\n",
    "                                                    'MentionedUsers',\n",
    "                                                    'ConversationId'])\n",
    "    \n",
    "    return ( tweets_df, replies_list )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "466a0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReplies(newsOutlet, convsID, start, end, maxTweets=2000):\n",
    "    '''\n",
    "    Extension of original scraper code\n",
    "    to pull all following conversations\n",
    "    \n",
    "    Inputs:\n",
    "         - newsOutlet: string, userId from orginal tweet post\n",
    "         - convsID: string, conversationId from original post\n",
    "         - start: string ('YYYY-MM-DD'), first date to include in search\n",
    "         - end: string ('YYYY-MM-DD'), first date to exclude in search\n",
    "         - maxTweets: int, cut off for search results\n",
    "     \n",
    "     Returns: tweets_list: list of lists,\n",
    "         representing each reply in the original thread.\n",
    "         Lowest level lists contain:\n",
    "             Datetime, TweetId, text, username, \n",
    "             newsOutlet, MentionedUsers, conversationId\n",
    "    '''\n",
    "       \n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list = []\n",
    "\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "\n",
    "    scraper_instance = sntwitter.TwitterSearchScraper(f'lang:en conversation_id:{convsID} ' + \n",
    "                                                      f'(filter:safe OR -filter:safe) -is:retweet')\n",
    "\n",
    "    \n",
    "    for i, tweet in enumerate(scraper_instance.get_items()):\n",
    "        if i>maxTweets:\n",
    "            break\n",
    "        \n",
    "        mentions = []\n",
    "\n",
    "        if tweet.id != tweet.conversationId:\n",
    "            if not tweet.mentionedUsers is None:\n",
    "                for user in tweet.mentionedUsers:\n",
    "                    mentions.append(user.username)\n",
    "            \n",
    "            text = tweet.content\n",
    "            text = text.replace(',', '')\n",
    "            text = text.replace('\"', '')\n",
    "            text = text.replace(\"'\", '')\n",
    "            \n",
    "            tweets_list.append([tweet.date,\n",
    "                                str(tweet.id), \n",
    "                                text, \n",
    "                                tweet.user.username,\n",
    "                                newsOutlet,\n",
    "                                \" \".join(mentions),\n",
    "                                str(tweet.conversationId)])\n",
    "    \n",
    "    \n",
    "    #scraper_instance._unset_guest_token()\n",
    "    \n",
    "    return tweets_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576204a3",
   "metadata": {},
   "source": [
    "## Step 4: Define your search ranges under \"Globals\"\n",
    "The START_DATE is included. The END_DATE is excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea4e8a",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "437a6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TWEETS = 50000\n",
    "START_DATE = '2021-01-01'\n",
    "END_DATE = '2021-02-01'\n",
    "\n",
    "FILE_SUFFIX = \"_Jan2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98747b3c",
   "metadata": {},
   "source": [
    "## Step 5: Run the scraper for each news source\n",
    "\n",
    "Due to issues with the scraper instance, I'm running each separately and saving out the results each time incase of interruptions. Then in the final step I'm merging dataframes.\n",
    "\n",
    "Each news source takes considerable time to run.\n",
    "An error of \"unable to find guest token\" may indicate an issue with the scraper instance.\n",
    "This happens frequently in retrieving replies.\n",
    "\n",
    "#### What to do when you get \"guest token\" error in replies:\n",
    "Processed conversation ids are recorded. If an error occurs, run the test code\n",
    "directly below to see if the full list of conversations were caputured.\n",
    "If not, run the replies code again until all have been saved.\n",
    "\n",
    "#### Immediate \"guest token\" errors without any scraper activity:\n",
    "Sometimes successive runs are strangely sticky.\n",
    "A short little request sometimes unsticks it.\n",
    "\n",
    "Try something like:\n",
    "getReplies(\"nytimes\", 1377405371279568898, START_DATE, END_DATE, 5)\n",
    "\n",
    "Or even just running something else like:\n",
    "print(MAX_TWEETS)\n",
    "\n",
    "For a few minutes until you get a response.\n",
    "Then resume.\n",
    "\n",
    "\n",
    "### Data Gathering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273a6de",
   "metadata": {},
   "source": [
    "#### 5.1: All Sources Original Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61ff9cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fox Posts:  (637, 6)\n"
     ]
    }
   ],
   "source": [
    "# pull fox news data\n",
    "fox_tweets, fox_conv_ids = getPosts(\"FoxNews\", \n",
    "                                   MAX_TWEETS, \n",
    "                                   START_DATE, \n",
    "                                   END_DATE)\n",
    "\n",
    "print(\"Fox Posts: \", fox_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04af62ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NY Times Posts:  (2647, 6)\n"
     ]
    }
   ],
   "source": [
    "# pull new york times post data\n",
    "nyt_tweets, nyt_conv_ids = getPosts(\"nytimes\", \n",
    "                                     MAX_TWEETS, \n",
    "                                     START_DATE, \n",
    "                                     END_DATE)\n",
    "\n",
    "print(\"NY Times Posts: \", nyt_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4674d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters Posts:  (13252, 6)\n"
     ]
    }
   ],
   "source": [
    "# pull reuters post data\n",
    "reuters_tweets, reuters_conv_ids = getPosts(\"Reuters\", \n",
    "                                     MAX_TWEETS, \n",
    "                                     START_DATE, \n",
    "                                     END_DATE)\n",
    "\n",
    "print(\"Reuters Posts: \", reuters_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a172869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging news sources\n",
    "posts_df = pd.concat([fox_tweets, nyt_tweets, reuters_tweets], ignore_index=True)\n",
    "\n",
    "# export dataframe into a CSV\n",
    "posts_df.to_csv(f'./data/tweets{FILE_SUFFIX}.csv',\n",
    "                index=False, quotechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059c178",
   "metadata": {},
   "source": [
    "#### 5.2: Replies Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e731af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up replies list\n",
    "replies = []\n",
    "processed_ids = []\n",
    "\n",
    "# Today\n",
    "if date.today().day < 10:\n",
    "    day = f'0{date.today().day}'\n",
    "else:\n",
    "    day = f'{date.today().day}'\n",
    "    \n",
    "if date.today().month < 10:\n",
    "    month = f'0{date.today().month}'\n",
    "else:\n",
    "    month = f'{date.today().month}'\n",
    "\n",
    "today = f'{date.today().year}-{month}-{day}'\n",
    "\n",
    "\n",
    "# if this is a continuation of a prior run\n",
    "# uncomment and run the below\n",
    "# proc_ids = pd.read_csv(\"./data/processed_ids.csv\")\n",
    "# processed_ids = proc_ids['conv_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f89f56",
   "metadata": {},
   "source": [
    "#### 5.2.1: New York Times Replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91b1cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(600)\n",
    "# add nyt replies \n",
    "# NOTE: if this breaks, run again until all ids are cleared\n",
    "\n",
    "for conv_id in nyt_conv_ids:\n",
    "    if not conv_id in processed_ids:\n",
    "\n",
    "        replies.extend(getReplies(\"nytimes\",\n",
    "                                  conv_id,\n",
    "                                  START_DATE,\n",
    "                                  today,\n",
    "                                  MAX_TWEETS))\n",
    "\n",
    "        processed_ids.append(conv_id)\n",
    "    \n",
    "        time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf95b2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NY Times replies retrieved. Move forward!\n"
     ]
    }
   ],
   "source": [
    "# Test for done-ness\n",
    "\n",
    "if len(nyt_conv_ids) == len(processed_ids):\n",
    "    print(\"All NY Times replies retrieved. Move forward!\")\n",
    "else:\n",
    "    print(f'{len(nyt_conv_ids) - len(processed_ids)} conversations ' +\n",
    "         'left to process. Run Again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6aabcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reply list to dataframe\n",
    "replies_df = pd.DataFrame(replies, columns=['Datetime',\n",
    "                                            'TweetId',\n",
    "                                            'Text',\n",
    "                                            'Username',\n",
    "                                            'NewsOutlet',\n",
    "                                            'MentionedUsers',\n",
    "                                            'ConversationId'])\n",
    "\n",
    "# export\n",
    "replies_df.to_csv(f'./data/replies{FILE_SUFFIX}_nyt.csv', sep=',', index=False)\n",
    "nyt = replies_df\n",
    "\n",
    "# clear\n",
    "replies = []\n",
    "processed_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47c12a72-418f-4a3d-b158-29e2705af169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique posts:  2647\n",
      "unique reply threads:  2357\n",
      "matches:  2647\n"
     ]
    }
   ],
   "source": [
    "nyt_articles = nyt.groupby('ConversationId')['TweetId'].count()\n",
    "nyt_articles = nyt_articles.reset_index()\n",
    "\n",
    "print(\"unique posts: \", nyt_tweets.shape[0])\n",
    "print(\"unique reply threads: \", nyt_articles.shape[0])\n",
    "print(\"matches: \", nyt_tweets.merge(nyt_articles, \n",
    "                                    on='ConversationId', \n",
    "                                    how='inner').shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85320e8e",
   "metadata": {},
   "source": [
    "#### 5.2.2: Reuters Replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bddf5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(600)\n",
    "# add reuters replies\n",
    "# NOTE: if this breaks, run again until all ids are cleared\n",
    "\n",
    "for conv_id in reuters_conv_ids:\n",
    "    if not conv_id in processed_ids:\n",
    "        replies.extend(getReplies(\"Reuters\",\n",
    "                                  conv_id,\n",
    "                                  START_DATE,\n",
    "                                  today,\n",
    "                                  MAX_TWEETS))\n",
    "\n",
    "        processed_ids.append(conv_id)\n",
    "    \n",
    "        time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "feca4fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Reuters replies retrieved. Move forward!\n"
     ]
    }
   ],
   "source": [
    "# test for done-ness again\n",
    "\n",
    "if len(reuters_conv_ids) == len(processed_ids):\n",
    "    print(\"All Reuters replies retrieved. Move forward!\")\n",
    "else:\n",
    "    print(f'{len(reuters_conv_ids) - len(processed_ids)} conversations ' +\n",
    "         'left to process. Run Again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38ebdbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reply list to dataframe\n",
    "replies_df = pd.DataFrame(replies, columns=['Datetime',\n",
    "                                            'TweetId',\n",
    "                                            'Text',\n",
    "                                            'Username',\n",
    "                                            'NewsOutlet',\n",
    "                                            'MentionedUsers',\n",
    "                                            'ConversationId'])\n",
    "\n",
    "# export\n",
    "replies_df.to_csv(f'./data/replies{FILE_SUFFIX}_reu.csv', sep=',', index=False)\n",
    "reu = replies_df\n",
    "\n",
    "#clear\n",
    "replies = []\n",
    "processed_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85713588-eae2-4c81-b3e9-1385bd5811db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique posts:  13252\n",
      "unique reply threads:  11920\n",
      "matches:  12373\n"
     ]
    }
   ],
   "source": [
    "reu_articles = reu.groupby('ConversationId')['TweetId'].count()\n",
    "reu_articles = reu_articles.reset_index()\n",
    "\n",
    "print(\"unique posts: \", reuters_tweets.shape[0])\n",
    "print(\"unique reply threads: \", reu_articles.shape[0])\n",
    "print(\"matches: \", reuters_tweets.merge(reu_articles, \n",
    "                                    on='ConversationId', \n",
    "                                    how='inner').shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102928f9",
   "metadata": {},
   "source": [
    "#### 5.2.3: Fox News Replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d24ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time.sleep(500)\n",
    "# add fox replies\n",
    "# NOTE: if this breaks, run again until all ids are cleared\n",
    "\n",
    "for conv_id in fox_conv_ids:\n",
    "    if not conv_id in processed_ids:\n",
    "        replies.extend(getReplies(\"FoxNews\",\n",
    "                                  conv_id,\n",
    "                                  START_DATE,\n",
    "                                  today,\n",
    "                                  MAX_TWEETS))\n",
    "\n",
    "        processed_ids.append(conv_id)\n",
    "    \n",
    "        time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0279316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Fox News replies retrieved. Move forward!\n"
     ]
    }
   ],
   "source": [
    "# test for done-ness again\n",
    "\n",
    "if (len(fox_conv_ids) == len(processed_ids)):\n",
    "    print(\"All Fox News replies retrieved. Move forward!\")\n",
    "else:\n",
    "    print(f'{(len(fox_conv_ids) - len(processed_ids))} conversations ' +\n",
    "         'left to process. Run Again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "204f080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reply list to dataframe\n",
    "replies_df = pd.DataFrame(replies, columns=['Datetime',\n",
    "                                            'TweetId',\n",
    "                                            'Text',\n",
    "                                            'Username',\n",
    "                                            'NewsOutlet',\n",
    "                                            'MentionedUsers',\n",
    "                                            'ConversationId'])\n",
    "\n",
    "# export\n",
    "replies_df.to_csv(f'./data/replies{FILE_SUFFIX}_fox.csv', sep=',', index=False)\n",
    "fox = replies_df\n",
    "\n",
    "# clear\n",
    "replies = []\n",
    "processed_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d746e159-325c-444c-ac9c-04cd35c80ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique posts:  637\n",
      "unique reply threads:  637\n",
      "matches:  637\n"
     ]
    }
   ],
   "source": [
    "fox_articles = fox.groupby('ConversationId')['TweetId'].count()\n",
    "fox_articles = fox_articles.reset_index()\n",
    "\n",
    "print(\"unique posts: \", fox_tweets.shape[0])\n",
    "print(\"unique reply threads: \", fox_articles.shape[0])\n",
    "print(\"matches: \", fox_tweets.merge(fox_articles, \n",
    "                                    on='ConversationId', \n",
    "                                    how='inner').shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfc32b",
   "metadata": {},
   "source": [
    "# Other notes and code snippets\n",
    "\n",
    "## If you need to stop during step 5. Save processed Ids and begin again later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "0550e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed ids incase of interrupted run\n",
    "proc_ids = pd.DataFrame({\"conv_id\": processed_ids})\n",
    "proc_ids.to_csv(\"./data/processed_ids.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56a47e",
   "metadata": {},
   "source": [
    "## For easy of use, you can join them all now (or later)\n",
    "...but without compression this file is too big to share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "fc7653d2-7273-44a1-9431-d20d7339d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-423-03dea9ebecbf>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset['Text'] = dataset['Text'].str.replace(r\"[,'\\\"]+\",\"\")\n",
      "<ipython-input-423-03dea9ebecbf>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset['Text'] = dataset['Text'].str.replace(r\"[\\r\\n]+\",\"\")\n"
     ]
    }
   ],
   "source": [
    "# remove stuff that breaks the file some times\n",
    "for dataset in [nyt, fox, reu]:\n",
    "    dataset['Text'] = dataset['Text'].str.replace(r\"[,'\\\"]+\",\"\")\n",
    "    dataset['Text'] = dataset['Text'].str.replace(r\"[\\r\\n]+\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ffa1a5-b100-4232-922e-66f1fcb893b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all if needed\n",
    "reu = pd.read_csv(f'./data/replies{FILE_SUFFIX}_reu.csv', quotechar='\"')\n",
    "fox = pd.read_csv(f'./data/replies{FILE_SUFFIX}_fox.csv', quotechar='\"')\n",
    "nyt = pd.read_csv(f'./data/replies{FILE_SUFFIX}_nyt.csv', quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "fc7da3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export all together\n",
    "all_replies = pd.concat([reu, fox, nyt], ignore_index=True)\n",
    "all_replies.to_csv(f'./data/replies{FILE_SUFFIX}.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
