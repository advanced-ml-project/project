{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65a6c23",
   "metadata": {},
   "source": [
    "# Using snscraper to Get Tweets\n",
    "\n",
    "## Step 1: Make sure you are running python v. 3.8 or higher.\n",
    "If you are not, to update python try:\n",
    "!conda upgrade notebook \n",
    "or \n",
    "!conda update jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4827188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.2\r\n"
     ]
    }
   ],
   "source": [
    "# check version\n",
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1ddbb",
   "metadata": {},
   "source": [
    "## Step 2: First time use requires installation of dev version of snsscrape\n",
    "Uncomment and run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c9c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4680430",
   "metadata": {},
   "source": [
    "## Step 3: Imports and Functions\n",
    "Please run the code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b5dd096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a898b",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2e4fec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosts(newsOutlet, maxTweets, start, end):\n",
    "    '''\n",
    "    Based on MartinBeckUT's python wrapper for snscraper.\n",
    "    Get all tweets and replies from March 2021.\n",
    "    '''     \n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list = []\n",
    "    replies_list = []\n",
    "\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i, tweet in enumerate(sntwitter\n",
    "                              .TwitterSearchScraper(f'from:{ newsOutlet } since:{start} ' +\n",
    "                                               f'until:{end} -is:retweet -is:reply')\n",
    "                              .get_items()):\n",
    "        if i>maxTweets:\n",
    "            break\n",
    "            \n",
    "        if tweet.mentionedUsers:\n",
    "            mentions = [user.username for user in tweet.mentionedUsers]\n",
    "        else:\n",
    "            mentions = []\n",
    "\n",
    "        tweets_list.append([tweet.date, \n",
    "                            tweet.id, \n",
    "                            tweet.content, \n",
    "                            tweet.user.username,\n",
    "                            mentions,\n",
    "                            tweet.conversationId])\n",
    "        \n",
    "        if tweet.conversationId not in replies_list:\n",
    "            replies_list.append(tweet.conversationId)\n",
    "    \n",
    "    # Creating a dataframes from the tweets lists above\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', \n",
    "                                                    'TweetId', \n",
    "                                                    'Text', \n",
    "                                                    'Username',\n",
    "                                                    'MentionedUsers',\n",
    "                                                    'ConversationId'])\n",
    "    \n",
    "    return ( tweets_df, replies_list )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "466a0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReplies(newsOutlet, convsID, start, end, maxTweets=2000):\n",
    "    '''\n",
    "    Extension of original scraper code\n",
    "    to pull all following conversations\n",
    "    '''\n",
    "       \n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list = []\n",
    "\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    scraper_instance = sntwitter.TwitterSearchScraper(f'lang:en conversation_id:{convsID} ' +\n",
    "                                                      f'since:{start} until:{end}')\n",
    "    \n",
    "    for i, tweet in enumerate(scraper_instance.get_items()):\n",
    "        if i>maxTweets:\n",
    "            break\n",
    "\n",
    "        if tweet.id != tweet.conversationId:\n",
    "            \n",
    "            if tweet.mentionedUsers:\n",
    "                mentions = [user.username for user in tweet.mentionedUsers]\n",
    "            else:\n",
    "                mentions = []\n",
    "            \n",
    "            tweets_list.append([tweet.date,\n",
    "                                tweet.id, \n",
    "                                tweet.content, \n",
    "                                tweet.user.username,\n",
    "                                newsOutlet,\n",
    "                                mentions,\n",
    "                                tweet.conversationId])\n",
    "    \n",
    "    \n",
    "    scraper_instance._unset_guest_token()\n",
    "    \n",
    "    return tweets_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09f97c",
   "metadata": {},
   "source": [
    "## Step 4: Define your search ranges under \"Globals\"\n",
    "The START_DATE is included. The END_DATE is excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea4e8a",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "437a6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TWEETS = 15000\n",
    "START_DATE = '2021-03-01'\n",
    "END_DATE = '2021-04-01'\n",
    "\n",
    "FILE_SUFFIX = \"_Mar2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98747b3c",
   "metadata": {},
   "source": [
    "## Step 5: Run the scraper for each news source\n",
    "\n",
    "Due to issues with the scraper instance, I'm running each separately and saving out the results each time incase of interruptions. Then in the final step I'm merging dataframes.\n",
    "\n",
    "Each news source takes considerable time to run.\n",
    "An error of \"unable to find guest token\" may indicate an issue with the scraper instance.\n",
    "This happens frequently in retrieving replies.\n",
    "\n",
    "### What to do when you get \"guest token\" error in replies:\n",
    "Processed conversation ids are recorded. If an error occurs, run the test code\n",
    "directly below to see if the full list of conversations were caputured.\n",
    "If not, run the replies code again until all have been saved.\n",
    "\n",
    "### Immediate \"guest token\" errors without any scraper activity:\n",
    "Sometimes successive runs are strangely sticky.\n",
    "A short little request sometimes unsticks it.\n",
    "\n",
    "Try something like:\n",
    "getReplies(\"nytimes\", 1377405371279568898, START_DATE, END_DATE, 5)\n",
    "\n",
    "Or even just running something else like:\n",
    "print(MAX_TWEETS)\n",
    "\n",
    "For a few minutes until you get a response.\n",
    "Then resume.\n",
    "\n",
    "\n",
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "61ff9cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fox Posts:  (841, 6)\n"
     ]
    }
   ],
   "source": [
    "# pull fox news data\n",
    "fox_tweets, fox_conv_ids = getPosts(\"FoxNews\", \n",
    "                                   MAX_TWEETS, \n",
    "                                   START_DATE, \n",
    "                                   END_DATE)\n",
    "\n",
    "print(\"Fox Posts: \", fox_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "875ac7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NY Times Posts:  (2682, 6)\n"
     ]
    }
   ],
   "source": [
    "# pull new york times post data\n",
    "nyt_tweets, nyt_conv_ids = getPosts(\"nytimes\", \n",
    "                                     MAX_TWEETS, \n",
    "                                     START_DATE, \n",
    "                                     END_DATE)\n",
    "\n",
    "print(\"NY Times Posts: \", nyt_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b2a3d32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters Posts:  (14410, 6)\n"
     ]
    }
   ],
   "source": [
    "# pull reuters post data\n",
    "reuters_tweets, reuters_conv_ids = getPosts(\"Reuters\", \n",
    "                                     MAX_TWEETS, \n",
    "                                     START_DATE, \n",
    "                                     END_DATE)\n",
    "\n",
    "print(\"Reuters Posts: \", reuters_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1e2daff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging news sources\n",
    "posts_df = pd.concat([fox_tweets, nyt_tweets, reuters_tweets], ignore_index=True)\n",
    "\n",
    "# export dataframe into a CSV\n",
    "posts_df.to_csv(f'./data/tweets{FILE_SUFFIX}.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5d424bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up replies list\n",
    "replies = []\n",
    "processed_ids = []\n",
    "\n",
    "# Today\n",
    "if date.today().day < 10:\n",
    "    day = f'0{date.today().day}'\n",
    "else:\n",
    "    day = f'{date.today().day}'\n",
    "    \n",
    "if date.today().month < 10:\n",
    "    month = f'0{date.today().month}'\n",
    "else:\n",
    "    month = f'{date.today().month}'\n",
    "\n",
    "today = f'{date.today().year}-{month}-{day}'\n",
    "\n",
    "\n",
    "# if this is a continuation of a prior run\n",
    "# uncomment and run the below\n",
    "# proc_ids = pd.read_csv(\"./data/processed_ids.csv\")\n",
    "# processed_ids = proc_ids['conv_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c2456955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nyt replies \n",
    "# NOTE: if this breaks, run again until all ids are cleared\n",
    "\n",
    "for conv_id in nyt_conv_ids:\n",
    "    if not conv_id in processed_ids:\n",
    "\n",
    "        replies.extend(getReplies(\"nytimes\",\n",
    "                                  conv_id,\n",
    "                                  START_DATE,\n",
    "                                  today))\n",
    "\n",
    "        processed_ids.append(conv_id)\n",
    "    \n",
    "        time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6621eabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NY Times replies retrieved. Move forward!\n"
     ]
    }
   ],
   "source": [
    "# Test for done-ness\n",
    "\n",
    "if len(nyt_conv_ids) == len(processed_ids):\n",
    "    print(\"All NY Times replies retrieved. Move forward!\")\n",
    "else:\n",
    "    print(f'{len(nyt_conv_ids) - len(processed_ids)} conversations ' +\n",
    "         'left to process. Run Again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3df794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reply list to dataframe\n",
    "replies_df = pd.DataFrame(replies, columns=['Datetime',\n",
    "                                            'TweetId',\n",
    "                                            'Text',\n",
    "                                            'Username',\n",
    "                                            'NewsOutlet',\n",
    "                                            'MentionedUsers',\n",
    "                                            'ConversationId'])\n",
    "\n",
    "# export\n",
    "replies_df.to_csv(f'./data/replies{FILE_SUFFIX}_nyt.csv', sep=',', index=False)\n",
    "\n",
    "# clear\n",
    "replies = []\n",
    "processed_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "8bddf5b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ScraperException",
     "evalue": "Unable to find guest token",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-284-1f1f1b461e6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreuters_conv_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconv_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         replies.extend(getReplies(\"Reuters\",\n\u001b[0m\u001b[1;32m      7\u001b[0m                                   \u001b[0mconv_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                   \u001b[0mSTART_DATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-fa14bd7df352>\u001b[0m in \u001b[0;36mgetReplies\u001b[0;34m(newsOutlet, convsID, start, end, maxTweets)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                       f'since:{start} until:{end}')\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscraper_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mmaxTweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36mget_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m                         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ext'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ext=mediaStats%2ChighlightedLabel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://api.twitter.com/2/search/adaptive.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaginationParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instructions_to_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_iter_api_data\u001b[0;34m(self, endpoint, params, paginationParams, cursor)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Retrieving scroll page {cursor}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreqParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_get_api_data\u001b[0;34m(self, endpoint, params)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_guest_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apiHeaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponseOkCallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_api_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_ensure_guest_token\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apiHeaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x-guest-token'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guestToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msnscrape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScraperException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unable to find guest token'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_unset_guest_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mScraperException\u001b[0m: Unable to find guest token"
     ]
    }
   ],
   "source": [
    "# add reuters replies\n",
    "# NOTE: if this breaks, run again until all ids are cleared\n",
    "\n",
    "for conv_id in reuters_conv_ids:\n",
    "    if not conv_id in processed_ids:\n",
    "        replies.extend(getReplies(\"Reuters\",\n",
    "                                  conv_id,\n",
    "                                  START_DATE,\n",
    "                                  today))\n",
    "\n",
    "        processed_ids.append(conv_id)\n",
    "    \n",
    "        time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "feca4fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10627 conversations left to process. Run Again.\n"
     ]
    }
   ],
   "source": [
    "# test for done-ness again\n",
    "\n",
    "if len(reuters_conv_ids) == len(processed_ids):\n",
    "    print(\"All Reuters replies retrieved. Move forward!\")\n",
    "else:\n",
    "    print(f'{len(reuters_conv_ids) - len(processed_ids)} conversations ' +\n",
    "         'left to process. Run Again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "e5aa361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reply list to dataframe\n",
    "replies_df = pd.DataFrame(replies, columns=['Datetime',\n",
    "                                            'TweetId',\n",
    "                                            'Text',\n",
    "                                            'Username',\n",
    "                                            'NewsOutlet',\n",
    "                                            'MentionedUsers',\n",
    "                                            'ConversationId'])\n",
    "\n",
    "# export\n",
    "replies_df.to_csv(f'./data/replies{FILE_SUFFIX}_reu.csv', sep=',', index=False)\n",
    "\n",
    "# clear\n",
    "#replies = []\n",
    "#processed_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8d24ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fox replies\n",
    "# NOTE: if this breaks, run again until all ids are cleared\n",
    "\n",
    "for conv_id in fox_conv_ids:\n",
    "    if not conv_id in processed_ids:\n",
    "        replies.extend(getReplies(\"FoxNews\",\n",
    "                                  conv_id,\n",
    "                                  START_DATE,\n",
    "                                  today))\n",
    "\n",
    "        processed_ids.append(conv_id)\n",
    "    \n",
    "        time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3255d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Fox News replies retrieved. Move forward!\n"
     ]
    }
   ],
   "source": [
    "# test for done-ness again\n",
    "\n",
    "if (len(fox_conv_ids) == len(processed_ids)):\n",
    "    print(\"All Fox News replies retrieved. Move forward!\")\n",
    "else:\n",
    "    print(f'{(len(fox_conv_ids) - len(processed_ids))} conversations ' +\n",
    "         'left to process. Run Again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8c077974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reply list to dataframe\n",
    "replies_df = pd.DataFrame(replies, columns=['Datetime',\n",
    "                                            'TweetId',\n",
    "                                            'Text',\n",
    "                                            'Username',\n",
    "                                            'NewsOutlet',\n",
    "                                            'MentionedUsers',\n",
    "                                            'ConversationId'])\n",
    "\n",
    "# export\n",
    "replies_df.to_csv(f'./data/replies{FILE_SUFFIX}_fox.csv', sep=',', index=False)\n",
    "\n",
    "# clear\n",
    "replies = []\n",
    "processed_ids = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ff8bc",
   "metadata": {},
   "source": [
    "## If you need to stop during step 5. Save processed Ids and begin again later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "0550e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed ids incase of interrupted run\n",
    "proc_ids = pd.DataFrame({\"conv_id\": processed_ids})\n",
    "proc_ids.to_csv(\"./data/processed_ids.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4906d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
